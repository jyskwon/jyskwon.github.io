---
layout: default
---


<!-- Main -->
<div id="main">
  <section style="max-width: 61em">
    <header class="major">
      <h2><strong>About</strong></h2>
      <!--p class="post-meta">{% if post.author %} • {{ post.author }}{% endif %}{% if page.meta %} • {{ page.meta }}{% endif %}</p-->
    </header>

    <section style="max-width: 65em">
      <strong>NEWS:</strong> Starting July 2021, I'll be joining the Department of <a href="https://ice.dgist.ac.kr/">Information and Communication Engineering</a> at <a href="https://dgist.ac.kr/kr/intro2020.html">DGIST</a> as an assistant professor.
      If you are interested in joining my lab <a href="https://diag.kr">DIAG</a>, please send your CV and transcript to diag.dgist@gmail.com. Thanks!

      <br><br/>
      Currently, I am a research assistant professor in the <a href="https://cs.kaist.ac.kr/">School of Computing</a> at <a href="https://www.kaist.ac.kr/html/en/index.html">KAIST</a>.
      I work with <a href="https://juhokim.com/">Prof. Juho Kim</a> in <a href="https://kixlab.github.io/">KIXLAB</a> and <a href="http://cps.kaist.ac.kr/~ishin/">Prof. Insik Shin</a> in <a href="http://cps.kaist.ac.kr/">CPS Lab</a>.

      <br/><br/>
      My research interests are focused on eliciting and leveraging diverse annotations from people
      to convert raw data to meaningful information,
      which can harness artificial intelligence systems and provide valuable insight to end users.
      <br/><br/>
      Keywords:
      <a href="https://www.explainxkcd.com/wiki/index.php/2173:_Trained_a_Neural_Net" target="_blank">#Human Computation</a>;
      <a href="https://hci.stanford.edu/publications/2013/CrowdWork/futureofcrowdwork-cscw2013.pdf" target="_blank">#Crowdsourcing</a>;
      <a href="https://interactions.acm.org/archive/view/march-april-2013/teaching-and-learning-human-computer-interaction" target="_blank">#Human-Computer Interaction</a>;
      <!--a href="https://www.forbes.com/sites/cognitiveworld/2019/06/26/the-present-and-future-of-computer-vision/" target="_blank">#Computer Vision</a>;-->
      <a href="https://www.csee.umbc.edu/courses/471/papers/turing.pdf" target="_blank">#Artificial Intelligence</a>
      <!--research interests look a bit too broad. Maybe talk about tool diversity, or something that shows the uniqueness of what you’re trying to do with research.-->
    </section>
  </section>

  <section style="max-width: 61em">
    <header class="major">
      <h2><strong>Publications</strong></h2>
    </header>
    <section style="max-width: 65em">
      <h3><strong>Conference and Journal Papers</strong></h3>
      <div class="row">
        <ul>
          <li>
            Zhefan Ye, <b>Jean Y. Song</b>, Zhiqiang Sui, Stephen Hart, Jorge Vilchis, Arbor, Walter S. Lasecki, and Odest C. Jenkins.
            <a href="#">Human-in-the-loop Pose Estimation via Shared Autonomy.
            </a> (to appear)
            In Proceedings of the ACM International Conference on Intelligent User Interfaces (IUI 2021).
          </li>
          <li>
            Stephan J. Lemmer, <b>Jean Y. Song</b>, and Jason J. Corso.
            <a href="#">Crowdsourcing More Effective Initializations for Single-target Trackers Through Automatic Re-querying.
            </a> (to appear)
            In Proceedings of the ACM/SIGCHI Conference on Human Factors in Computing Systems (CHI 2021).
          </li>
          <li>
            Yoonjoo Lee, John Joon Young Chung, <b>Jean Y. Song</b>, Minsuk Chang, and Juho Kim.
            <a href="#">Personalizing Ambience and Illusionary Presence: How People Use "Study with Me" Videos to Create Effective Studying Environments.
            </a> (to appear)
            In Proceedings of the ACM/SIGCHI Conference on Human Factors in Computing Systems (CHI 2021).
          </li>
          <li>
            <b>Jean Y. Song</b>, John Joon Young Chung, David F. Fouhey, and Walter S. Lasecki.
            <a href="/publications/CReference_CSCW2020.pdf">C-Reference: Improving 2D to 3D Object Pose Estimation Accuracy via Crowdsourced Joint Object Estimation.
            </a>
            In Proceedings of the ACM International Conference on Computer Supported Cooperative Work and Social Computing (CSCW 2020).
          </li>
          <li>
            Divya Ramesh, Anthony Z. Liu, Andres J. Echeverria, <b>Jean Y. Song</b>, Nicholas R. Waytowich, and Walter S. Lasecki.
            <a href="/publications/contrast_effect_AAMAS2020.pdf">Yesterday’s Reward is Today’s Punishment: Contrast Effects in Human Feedback to Reinforcement Learning Agents.
            </a>
            In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2020).
            <font color="#FFC300">Pragnesh Jay Modi Best Student Paper</font>
          </li>
          <li>
            Yan Chen, Maulishree Pandey, <b>Jean Y. Song</b>, Walter S. Lasecki, and Steve Oney.
            <a href="/publications/GUITesting_CHI2020.pdf">Improving Crowd-Supported GUI Testing with Structural Guidance.
            </a>
            In Proceedings of the ACM/SIGCHI Conference on Human Factors in Computing Systems (CHI 2020).
          </li>
          <li>John Joon Young Chung, <b>Jean Y. Song</b>, Sindhu Kutty, Sungsoo (Ray) Hong, Juho Kim, and Walter S. Lasecki.
            <a href="/publications/ManyThoughts_CSCW2019.pdf">Efficient Elicitation Approaches to Estimate Collective Crowd Answers.
            </a>
            In Proceedings of the ACM International Conference on Computer Supported Cooperative Work and Social Computing (CSCW 2019).
            Austin, TX. <font color="#FFC300">Best Paper Honorable Mention</font>
          </li>
          <li><b>Jean Y. Song</b>, Raymond Fok, Juho Kim, and Walter S. Lasecki.
            <a href="https://doi.org/10.1145/3237188">FourEyes:
              Leveraging Tool Diversity as a Means to Improve Aggregate Accuracy in Crowdsourcing.
            </a>
            In ACM Transactions on Interactive Intelligent Systems, Volume 19, Issue 1, Article No. 3 (TiiS 2019).
          </li>
          <li><b>Jean Y. Song</b>, Stephan J. Lemmer, Michael Xieyang Liu, Shiyan Yan, Juho Kim, Jason J. Corso, and Walter S. Lasecki.
            <a href="/publications/popup_IUI2019.pdf">Popup:
              Reconstructing 3D Video Using Particle Filtering to Aggregate Crowd Responses.
            </a>
            In Proceedings of the ACM International Conference on Intelligent User Interfaces (IUI 2019).
            Los Angeles, CA. <!--[25% Acceptance Rate]-->
          <li> <b>Jean Y. Song</b>, Raymond Fok, Alan Lundgard, Fan Yang, Juho Kim, and Walter S. Lasecki.
            <a href="/publications/IUI2018_FourEyes.pdf">Two Tools are Better Than One:
              Tool Diversity as a Means of Improving Aggregate Crowd Performance.
            </a>
            In Proceedings of the ACM International Conference on Intelligent User Interfaces (IUI 2018).
            Tokyo, Japan. <!--[23% Acceptance Rate]--><font color="#FFC300">Best Student Paper Honorable Mention</font>
          </li>
        </ul>
      </div>
      <h3><strong>Posters, Demos, and Workshop Papers</strong></h3>
      <div class="row">
        <ul>
          <li>Andrew M. Vernier, <b>Jean Y. Song</b>, Edward Sun, Allison Kench, and Walter S. Lasecki.
            <a href="/publications/corsica_UIST2019-poster.pdf">Towards Universal Evaluation of Image Annotation Interfaces.
            </a>
            In Proceedings of the ACM Symposium on User Interface Software and Technology (UIST 2019).
            New Orleans, LA.
          </li>
          <li> <b>Jean Y. Song</b>, Raymond Fok, Fan Yang, Kyle Wang, Alan Lundgard, and Walter S. Lasecki.
            <a href="/publications/GroupSight2017_ToolDiversity.pdf">Tool Diversity
              as a Means of Improving Aggregate Crowd Performance on Image Segmentation Tasks.
            </a>
            In HCOMP Workshop on Human Computation for Image and Video Analysis (HCOMP GroupSight 2017).
            Quebec City, Quebec. 2017.
          </li>
          <li> Sai R. Gouravajhala, <b>Jean Y. Song</b>, Jinyeong Yim, Raymond Fok, Yanda Huang, Fan Yang, Kyle Wang, Yilei An, and Walter S. Lasecki.
            <a href="/publications/eureca_ci2017.pdf">Towards Hybrid Intelligence for Robotics.
            </a>
            In Collective Intelligence Conference (CI 2017). New York, NY.
          </li>
          <li> <b>Jean Y. Song</b> and Charles R. Meyer.
            <a href="/publications/Jean_SPIE_Poster.pdf">
              2D-3D Image Registration using Thin-Plate Spline and Volume Rendering.
            </a>
            SPIE Medical Imaging 2015.
            Orlando, FL.
          </li>
          <li> <b>Jean Y. Song</b>, Jeffrey A. Fessler, and Charles R. Meyer.
            <a href="/publications/Jean_CR_final.pdf">
              Adaptive Filtering on Conditional Mutual Information for Intermodal Non-Rigid Image Registration.
            </a>
            IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC 2014).
            Seattle, WA.
          </li>
        </ul>
      </div>
      <h3><strong>Thesis</strong></h3>
      <div class="row">
        <ul>
          <li><b>Jean Y. Song</b>.
            <a href="/publications/Thesis2019_JeanYSong.pdf">Eliciting and Leveraging Input Diversity in Crowd-Powered Intelligent Systems.
            </a>
            University of Michigan Ph.D. Thesis. 2019.
          </li>
        </ul>
      </div>
    </section>
  </section>

  <section style="max-width: 65em">
    <header class="major">
      <h2><strong>Projects</strong></h2>
    </header>
    <div class="row">
      <header style="width: 100%">
        <h3>Robust 4D Simulation of Rare Events enabled by Human-Augmented Computer Vision</h3>
      </header>
      <div style="max-width: 36em; float: left">
        Research in robotics and autonomous vehicles suffers from a lack of realistic training data and environments
        in which to test new approaches. Rare and unusual events such as traffic accidents occur several orders of
        magnitude less frequently than is needed to collect large enough training and testing sets, presenting a
        fundamental bottleneck in the research and deployment of such systems. Thus, we propose to use a crowdsourced
        human-­in-­the-­loop approach to guide computer vision algorithms to extract measurement information from large
        video corpora, allowing us to create simulations of scene dynamics for training and testing.
      </div>
      <div class="project-image">
        <img src="/images/projects/intro_application.png">
      </div>
    </div><br /><br />

    <div class="row">
      <header style="width: 100%">
        <h3>Crowdsourcing Emotion, Intention, and Context Annotations from Dialog Videos</h3>
      </header>
      <div style="max-width: 36em; float: left">
        Dialog videos contain rich contextual, emotional, and intentional cues of the characters and their surroundings.
        In this project, we aim to build a crowdsourcing platform that collects these information from a large dialog video
        dataset. The collection and aggregation process can be challenging because the temporal dimension of the dataset
        has to be considered, and the labels can be highly subjective. We combat these challenges
        by exploring crowdsourcing techniques to design workflows and answer aggregation methods that efficiently collects
        multi-dimensional labels and overcome the subjective nature of the collected annotations.
      </div>
      <div class="project-image">
        <img src="/images/projects/comingsoon.png">
      </div>
    </div><br /><br />

    <div class="row">
      <header style="width: 100%">
        <h3>Improving Aggregate Crowd Performance on Crowd-Assisted Image Segmentation</h3>
      </header>
      <div style="max-width: 36em; float: left">
        In designing crowdsourcing tasks, we want to achieve as high accuracy as possible from the given
        resources. In this work, we introduced an approach of leveraging tool diversity as a means of improving aggregate
        crowd performance. We define tool diversity as a property of a system (or a task), that enables to use
        different tools for a same task. In semantic image segmentation tasks, we show that our approach
        improves the aggregate accuracy significantly, compared to using a single best tool alone.
      </div>

      <div class="project-image">
        <img src="/images/projects/2018-tooldiversity.png">
      </div>
    </div><br /><br />

    <!--div class="row">
      <header style="width: 100%">
        <h3>Crowd-Assisted Robotics</h3>
      </header>
      <div style="max-width: 36em; float: left">
        We are building crowdsourcing tools to help autonomous robots recognize new contexts or problems in real-time.
        Our system uses a hybrid intelligent workflow that combines human intelligence from the crowd with automated
        support in the form of focused tasks (ones that the system is not able to complete on its own) and smart tools
        for aiding object segmentation. <This uses the machine’s ability to precisely select content with people’s semantic
          understanding of the scene. It also allows us to benefit from as much automated labeling as can be done reliably,
          while using human intelligence to both fill in the gaps, and ensure that new objects in a scene do not result
          in failures to complete an assigned task.
        </div>
        <div class="project-image">
          <img src="/images/projects/2017-CI.png">
        </div>
      </div><br /><br /-->

      <!--div class="row">
      <header style="width: 100%">
      <h3>Intermodal Non-rigid Image Registration</h3>
    </header>
    <div style="max-width: 50em; float: left">
    I've also collaborated with Prof. (Emeritus) Charles R. Meyer in the Department of Biomedical Engineering and Prof. Jeffrey A. Fessler in EECS.
    I worked on medical image registration such as intermodal non-rigid image registration based on mutual information, and 2D-3D projection registration.<br />
    <ul>
    <li> Jean Y. Song and Charles R. Meyer, <a href="/publications/Jean_SPIE_Poster.pdf">2D-3D Image Registration using Thin-Plate Spline and Volume Rendering</a>,
    SPIE Medical Imaging 2015, Orlando, Feb. 2015. </li>
    <li> Jean Y. Song, J. A. Fessler, and C. R. Meyer, <a href="/publications/Jean_CR_final.pdf">Adaptive Filtering on Conditional Mutual Information for
    Intermodal Non-Rigid Image Registration</a>, IEEE NSS/MIC 2014, Seattle, Nov. 2014. </li>
  </ul>
</div>
</div-->
  </section>
</div>
